{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advanced Text Processing System\n",
    "-------------------------------\n",
    "A comprehensive NLP system that provides:\n",
    "- N-gram language modeling\n",
    "- Text generation\n",
    "- Spelling correction with multiple methods\n",
    "- Auto-completion\n",
    "- Web interface using Gradio\n",
    "\n",
    "Authors: El Guelta Mohamad Saber , El Hadifi Soukaina\n",
    "Date: April 13, 2025\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "import os\n",
    "import gradio as gr\n",
    "from collections import defaultdict, Counter\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Main class handling all text processing functionality.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus_path, keyboard_graph_path, ngram_size=2, smoothing_k=0.1, min_frequency=2):\n",
    "        \"\"\"\n",
    "        Initialize the text processor with necessary parameters and paths.\n",
    "\n",
    "        Args:\n",
    "            corpus_path: Path to the text corpus\n",
    "            keyboard_graph_path: Path to the keyboard layout graph\n",
    "            ngram_size: Size of n-grams to use (default: 2 for bigrams)\n",
    "            smoothing_k: Smoothing parameter for add-k smoothing (default: 0.1)\n",
    "            min_frequency: Minimum word frequency for dictionary inclusion (default: 2)\n",
    "        \"\"\"\n",
    "        self.ngram_size = ngram_size\n",
    "        self.smoothing_k = smoothing_k\n",
    "\n",
    "        print(\"Loading data...\")\n",
    "        self.dictionary, self.word_frequencies = self._create_dictionary_from_corpus(corpus_path, min_frequency)\n",
    "        self.keyboard_graph = self._load_keyboard_graph(keyboard_graph_path)\n",
    "\n",
    "        print(\"Training the n-gram model...\")\n",
    "        sample_file = self._extract_sample_for_training(corpus_path)\n",
    "        self.ngram_counts = self._train(sample_file, self.dictionary, ngram_size, smoothing_k)\n",
    "        os.unlink(sample_file)  # Cleanup\n",
    "\n",
    "        print(f\"Model ready! {len(self.ngram_counts)} contexts in the model.\")\n",
    "\n",
    "    # ========== DATA PREPARATION METHODS ==========\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and tokenize text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text string\n",
    "\n",
    "        Returns:\n",
    "            List of tokens\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        return text.split()\n",
    "\n",
    "    def _create_dictionary_from_corpus(self, corpus_file, min_frequency=2):\n",
    "        \"\"\"\n",
    "        Create a dictionary from frequent words in the corpus.\n",
    "\n",
    "        Args:\n",
    "            corpus_file: Path to corpus file\n",
    "            min_frequency: Minimum count for a word to be included\n",
    "\n",
    "        Returns:\n",
    "            A tuple with (dictionary set, word frequency counter)\n",
    "        \"\"\"\n",
    "        word_counts = Counter()\n",
    "\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                words = self._preprocess_text(line)\n",
    "                word_counts.update(words)\n",
    "\n",
    "        # Only keep words that appear at least min_frequency times\n",
    "        dictionary = {word for word, count in word_counts.items() if count >= min_frequency}\n",
    "        return dictionary, word_counts\n",
    "\n",
    "    def _extract_sample_for_training(self, corpus_file, sample_size=100000):\n",
    "        \"\"\"\n",
    "        Extract a sample from corpus for faster training.\n",
    "\n",
    "        Args:\n",
    "            corpus_file: Path to corpus file\n",
    "            sample_size: Number of characters to sample\n",
    "\n",
    "        Returns:\n",
    "            Path to temporary file with sample\n",
    "        \"\"\"\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            sample_text = f.read(sample_size)\n",
    "\n",
    "        temp_sample = tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8')\n",
    "        temp_sample.write(sample_text)\n",
    "        temp_sample.close()\n",
    "        return temp_sample.name\n",
    "\n",
    "    def _prepare_data(self, infile, vocab):\n",
    "        \"\"\"\n",
    "        Read and prepare data for training.\n",
    "\n",
    "        Args:\n",
    "            infile: Path to input file\n",
    "            vocab: Dictionary of valid words\n",
    "\n",
    "        Returns:\n",
    "            List of processed tokens\n",
    "        \"\"\"\n",
    "        with open(infile, 'r', encoding='utf-8') as f:\n",
    "            tokens = self._preprocess_text(f.read())\n",
    "        tokens = [w if w in vocab else '<UNK>' for w in tokens]\n",
    "        return ['<s>'] + tokens + ['</s>']\n",
    "\n",
    "    # ========== LANGUAGE MODEL METHODS ==========\n",
    "\n",
    "    def _train_worker(self, tokens, start, end, ngram_size):\n",
    "        \"\"\"\n",
    "        Worker function for parallel n-gram counting.\n",
    "\n",
    "        Args:\n",
    "            tokens: List of tokens\n",
    "            start: Start index\n",
    "            end: End index\n",
    "            ngram_size: Size of n-grams\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (ngram_counts, total_counts)\n",
    "        \"\"\"\n",
    "        local_ngram_counts = defaultdict(Counter)\n",
    "        local_total_counts = defaultdict(int)\n",
    "        for i in range(start, end - ngram_size + 1):\n",
    "            context = tuple(tokens[i:i + ngram_size - 1])\n",
    "            word = tokens[i + ngram_size - 1]\n",
    "            local_ngram_counts[context][word] += 1\n",
    "            local_total_counts[context] += 1\n",
    "        return local_ngram_counts, local_total_counts\n",
    "\n",
    "    def _train(self, infile, vocab, ngram_size=2, smoothing_k=0.1):\n",
    "        \"\"\"\n",
    "        Train the n-gram language model with parallel processing.\n",
    "\n",
    "        Args:\n",
    "            infile: Path to input file\n",
    "            vocab: Dictionary of valid words\n",
    "            ngram_size: Size of n-grams\n",
    "            smoothing_k: Parameter for add-k smoothing\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of n-gram probabilities\n",
    "        \"\"\"\n",
    "        tokens = self._prepare_data(infile, vocab)\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "        chunk_size = len(tokens) // n_workers\n",
    "        args = [(tokens, i, min(i + chunk_size, len(tokens)), ngram_size) for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "        with multiprocessing.Pool(n_workers) as pool:\n",
    "            results = pool.starmap(self._train_worker, args)\n",
    "\n",
    "        ngram_counts = defaultdict(Counter)\n",
    "        total_counts = defaultdict(int)\n",
    "        vocab_size = len(vocab) + 1  # To include <UNK>\n",
    "\n",
    "        for local_ngram_counts, local_total_counts in results:\n",
    "            for context, words in local_ngram_counts.items():\n",
    "                ngram_counts[context].update(words)\n",
    "                total_counts[context] += local_total_counts[context]\n",
    "\n",
    "        # Apply smoothing\n",
    "        for context in ngram_counts:\n",
    "            total = total_counts[context] + smoothing_k * vocab_size\n",
    "            ngram_counts[context] = {word: (count + smoothing_k) / total\n",
    "                                     for word, count in ngram_counts[context].items()}\n",
    "        return ngram_counts\n",
    "\n",
    "    def predict_sentence_probability(self, sentence):\n",
    "        \"\"\"\n",
    "        Calculate probability of a sentence given the model.\n",
    "\n",
    "        Args:\n",
    "            sentence: Input sentence\n",
    "\n",
    "        Returns:\n",
    "            Log probability of the sentence\n",
    "        \"\"\"\n",
    "        tokens = ['<s>'] + self._preprocess_text(sentence) + ['</s>']\n",
    "        prob = 0\n",
    "        for i in range(len(tokens) - self.ngram_size + 1):\n",
    "            context = tuple(tokens[i:i + self.ngram_size - 1])\n",
    "            word = tokens[i + self.ngram_size - 1]\n",
    "            prob += self.ngram_counts.get(context, {}).get(word, np.log(1e-10))\n",
    "        return prob\n",
    "\n",
    "    def test_perplexity(self, test_file):\n",
    "        \"\"\"\n",
    "        Calculate perplexity of a test corpus.\n",
    "\n",
    "        Args:\n",
    "            test_file: Path to test file\n",
    "\n",
    "        Returns:\n",
    "            Perplexity score\n",
    "        \"\"\"\n",
    "        tokens = self._prepare_data(test_file, self.dictionary)\n",
    "        sentence_probs = [self.predict_sentence_probability(' '.join(tokens[i:i+10]))\n",
    "                         for i in range(0, len(tokens), 10)]\n",
    "        return np.exp(-np.mean(sentence_probs) / 10)\n",
    "\n",
    "    # ========== TEXT GENERATION METHODS ==========\n",
    "\n",
    "    def generate_text(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Generate text using the trained model.\n",
    "\n",
    "        Args:\n",
    "            max_length: Maximum length of generated text in words\n",
    "\n",
    "        Returns:\n",
    "            Generated text string\n",
    "        \"\"\"\n",
    "        sentence = ['<s>']\n",
    "        while len(sentence) < max_length and sentence[-1] != '</s>':\n",
    "            context = tuple(sentence[-(self.ngram_size - 1):])\n",
    "            if context not in self.ngram_counts:\n",
    "                break\n",
    "\n",
    "            words = list(self.ngram_counts[context].keys())\n",
    "            probs = list(self.ngram_counts[context].values())\n",
    "\n",
    "            # Normalize probabilities to sum to 1\n",
    "            probs_sum = sum(probs)\n",
    "            if probs_sum > 0:\n",
    "                probs = [p/probs_sum for p in probs]\n",
    "\n",
    "            sentence.append(np.random.choice(words, p=probs))\n",
    "\n",
    "        return ' '.join(sentence[1:-1])\n",
    "\n",
    "    def generate_continuation(self, text, max_length=20):\n",
    "        \"\"\"\n",
    "        Generate a continuation of the input text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text to continue\n",
    "            max_length: Maximum length of generated continuation\n",
    "\n",
    "        Returns:\n",
    "            Original text plus generated continuation\n",
    "        \"\"\"\n",
    "        words = ['<s>'] + self._preprocess_text(text)\n",
    "\n",
    "        # Create an initial context from the last words\n",
    "        context_size = self.ngram_size - 1\n",
    "        if len(words) >= context_size:\n",
    "            context = tuple(words[-context_size:])\n",
    "        else:\n",
    "            # Padding if necessary\n",
    "            context = tuple(['<s>'] * (context_size - len(words)) + words)\n",
    "\n",
    "        # Generate the continuation\n",
    "        continuation = []\n",
    "        for _ in range(max_length):\n",
    "            if context not in self.ngram_counts:\n",
    "                break\n",
    "\n",
    "            words_list = list(self.ngram_counts[context].keys())\n",
    "            probs_list = list(self.ngram_counts[context].values())\n",
    "\n",
    "            # Normalize probabilities\n",
    "            probs_sum = sum(probs_list)\n",
    "            if probs_sum > 0:\n",
    "                probs_list = [p/probs_sum for p in probs_list]\n",
    "\n",
    "            next_word = np.random.choice(words_list, p=probs_list)\n",
    "\n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "\n",
    "            continuation.append(next_word)\n",
    "\n",
    "            # Update the context for the next word\n",
    "            context = context[1:] + (next_word,)\n",
    "\n",
    "        return text + \" \" + \" \".join(continuation)\n",
    "\n",
    "    def auto_complete(self, text):\n",
    "        \"\"\"\n",
    "        Predict the next word after the input text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "\n",
    "        Returns:\n",
    "            Most likely next word\n",
    "        \"\"\"\n",
    "        tokens = self._preprocess_text(text)\n",
    "        context = tuple(tokens[-(self.ngram_size - 1):])\n",
    "        predictions = self.ngram_counts.get(context, {})\n",
    "\n",
    "        if predictions:\n",
    "            word, _ = max(predictions.items(), key=lambda item: item[1])\n",
    "            return word\n",
    "        else:\n",
    "            return random.choice(list(self.dictionary))\n",
    "\n",
    "    def autocomplete_text(self, text, num_words=5):\n",
    "        \"\"\"\n",
    "        Auto-complete text with multiple words.\n",
    "\n",
    "        Args:\n",
    "            text: Input text\n",
    "            num_words: Number of words to add\n",
    "\n",
    "        Returns:\n",
    "            Completed text\n",
    "        \"\"\"\n",
    "        current_text = text\n",
    "        result = current_text\n",
    "\n",
    "        for _ in range(num_words):\n",
    "            next_word = self.auto_complete(current_text)\n",
    "            result += \" \" + next_word\n",
    "            current_text += \" \" + next_word\n",
    "\n",
    "        return result\n",
    "\n",
    "    # ========== SPELLING CORRECTION METHODS ==========\n",
    "\n",
    "    def _load_keyboard_graph(self, file_path):\n",
    "        \"\"\"\n",
    "        Load keyboard layout graph for spelling correction.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to keyboard graph file\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of adjacent keys\n",
    "        \"\"\"\n",
    "        adjacency = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                keys = line.strip().split()\n",
    "                adjacency[keys[0]] = set(keys[1:])\n",
    "        return adjacency\n",
    "\n",
    "    def _soundex(self, word):\n",
    "        \"\"\"\n",
    "        Compute Soundex code for phonetic matching.\n",
    "\n",
    "        Args:\n",
    "            word: Input word\n",
    "\n",
    "        Returns:\n",
    "            Soundex code string\n",
    "        \"\"\"\n",
    "        soundex_dict = {\n",
    "            \"bfpv\": \"1\", \"cgjkqsxz\": \"2\", \"dt\": \"3\",\n",
    "            \"l\": \"4\", \"mn\": \"5\", \"r\": \"6\"\n",
    "        }\n",
    "        # Normalize the word and initialize with the first letter\n",
    "        word = word.lower()\n",
    "        first_letter = word[0]\n",
    "        encoded = first_letter.upper()\n",
    "        # Dictionary to map each letter to the corresponding Soundex number\n",
    "        letter_to_code = {}\n",
    "        for chars, code in soundex_dict.items():\n",
    "            for char in chars:\n",
    "                letter_to_code[char] = code\n",
    "        # Iterate through the rest of the characters\n",
    "        for char in word[1:]:\n",
    "            if char in letter_to_code:\n",
    "                code = letter_to_code[char]\n",
    "                if encoded[-1] != code:  # Prevent consecutive identical codes\n",
    "                    encoded += code\n",
    "        # Ensure the code is exactly 4 characters long\n",
    "        encoded = encoded.ljust(4, \"0\")[:4]\n",
    "        return encoded\n",
    "\n",
    "    def _levenshtein_distance(self, s1, s2, keyboard_graph=None):\n",
    "        \"\"\"\n",
    "        Calculate Levenshtein edit distance between strings.\n",
    "\n",
    "        Args:\n",
    "            s1: First string\n",
    "            s2: Second string\n",
    "            keyboard_graph: Optional keyboard layout for weighted distance\n",
    "\n",
    "        Returns:\n",
    "            Edit distance as an integer\n",
    "        \"\"\"\n",
    "        len_s1, len_s2 = len(s1), len(s2)\n",
    "        dp = np.zeros((len_s1 + 1, len_s2 + 1))\n",
    "\n",
    "        for i in range(len_s1 + 1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(len_s2 + 1):\n",
    "            dp[0][j] = j\n",
    "\n",
    "        for i in range(1, len_s1 + 1):\n",
    "            for j in range(1, len_s2 + 1):\n",
    "                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "\n",
    "                if keyboard_graph and s1[i - 1] in keyboard_graph and s2[j - 1] in keyboard_graph[s1[i - 1]]:\n",
    "                    cost = 0.5  # Adjust cost for adjacent keys\n",
    "\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + 1,  # Deletion\n",
    "                    dp[i][j - 1] + 1,  # Insertion\n",
    "                    dp[i - 1][j - 1] + cost  # Substitution\n",
    "                )\n",
    "\n",
    "                if i > 1 and j > 1 and s1[i - 1] == s2[j - 2] and s1[i - 2] == s2[j - 1]:\n",
    "                    dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)  # Transposition\n",
    "\n",
    "        return int(dp[len_s1, len_s2])\n",
    "\n",
    "    def _correct_word(self, word, dictionary=None, keyboard_graph=None, k=5):\n",
    "        \"\"\"\n",
    "        Basic spell correction using edit distance.\n",
    "\n",
    "        Args:\n",
    "            word: Word to correct\n",
    "            dictionary: Dictionary to use (default is self.dictionary)\n",
    "            keyboard_graph: Keyboard graph (default is self.keyboard_graph)\n",
    "            k: Number of suggestions to return\n",
    "\n",
    "        Returns:\n",
    "            List of k closest corrections\n",
    "        \"\"\"\n",
    "        dictionary = dictionary or self.dictionary\n",
    "        keyboard_graph = keyboard_graph or self.keyboard_graph\n",
    "\n",
    "        candidates = [(dict_word, self._levenshtein_distance(word, dict_word, keyboard_graph))\n",
    "                      for dict_word in dictionary]\n",
    "        candidates = sorted(candidates, key=lambda x: x[1])\n",
    "        return [word for word, _ in candidates[:k]]\n",
    "\n",
    "    def _smart_correct(self, word, k=5):\n",
    "        \"\"\"\n",
    "        Optimized correction with initial filtering.\n",
    "\n",
    "        Args:\n",
    "            word: Word to correct\n",
    "            k: Number of suggestions to return\n",
    "\n",
    "        Returns:\n",
    "            List of k closest corrections\n",
    "        \"\"\"\n",
    "        filtered_dict = {w for w in self.dictionary if abs(len(w) - len(word)) <= 2 and w[0] == word[0]}\n",
    "        return self._correct_word(word, filtered_dict, self.keyboard_graph, k)\n",
    "\n",
    "    def _phonetic_correct(self, word, k=5):\n",
    "        \"\"\"\n",
    "        Phonetic correction using Soundex.\n",
    "\n",
    "        Args:\n",
    "            word: Word to correct\n",
    "            k: Number of suggestions to return\n",
    "\n",
    "        Returns:\n",
    "            List of k closest corrections\n",
    "        \"\"\"\n",
    "        word_soundex = self._soundex(word)\n",
    "        filtered_dict = {w for w in self.dictionary if self._soundex(w) == word_soundex}\n",
    "        return self._correct_word(word, filtered_dict, self.keyboard_graph, k)\n",
    "\n",
    "    def get_best_correction(self, word):\n",
    "        \"\"\"\n",
    "        Get best spelling correction by combining methods.\n",
    "\n",
    "        Args:\n",
    "            word: Word to correct\n",
    "\n",
    "        Returns:\n",
    "            Best correction suggestion\n",
    "        \"\"\"\n",
    "        suggestions = set(self._smart_correct(word) +\n",
    "                          self._phonetic_correct(word) +\n",
    "                          self._correct_word(word))\n",
    "\n",
    "        suggestions_with_scores = [(s, self._levenshtein_distance(word, s, self.keyboard_graph) -\n",
    "                                    0.1 * np.log(self.word_frequencies.get(s, 1)))\n",
    "                                   for s in suggestions]\n",
    "\n",
    "        return min(suggestions_with_scores, key=lambda x: x[1])[0] if suggestions_with_scores else word\n",
    "\n",
    "    # ========== PUBLIC API METHODS ==========\n",
    "\n",
    "    def correct_text(self, text):\n",
    "        \"\"\"\n",
    "        Correct spelling errors in text.\n",
    "\n",
    "        Args:\n",
    "            text: Input text with potential errors\n",
    "\n",
    "        Returns:\n",
    "            Corrected text\n",
    "        \"\"\"\n",
    "        words = self._preprocess_text(text)\n",
    "        corrected_words = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.dictionary:\n",
    "                corrected_words.append(word)\n",
    "            else:\n",
    "                corrected_word = self.get_best_correction(word)\n",
    "                corrected_words.append(corrected_word)\n",
    "\n",
    "        return ' '.join(corrected_words)\n",
    "\n",
    "    def correct_and_autocomplete(self, text, num_words=5):\n",
    "        \"\"\"\n",
    "        Correct text and predict its continuation.\n",
    "\n",
    "        Args:\n",
    "            text: Input text with potential errors\n",
    "            num_words: Number of words to add\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing original, corrected, and completed text\n",
    "        \"\"\"\n",
    "        corrected_text = self.correct_text(text)\n",
    "        completed_text = self.autocomplete_text(corrected_text, num_words)\n",
    "\n",
    "        return {\n",
    "            \"original_text\": text,\n",
    "            \"corrected_text\": corrected_text,\n",
    "            \"completed_text\": completed_text\n",
    "        }\n",
    "\n",
    "\n",
    "class TextProcessorApp:\n",
    "    \"\"\"Gradio interface for the TextProcessor system.\"\"\"\n",
    "\n",
    "    def __init__(self, processor):\n",
    "        \"\"\"\n",
    "        Initialize the app with a TextProcessor instance.\n",
    "\n",
    "        Args:\n",
    "            processor: TextProcessor instance\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.demo = self._create_interface()\n",
    "\n",
    "    def _create_interface(self):\n",
    "        \"\"\"Create the Gradio interface.\"\"\"\n",
    "        demo = gr.Blocks(title=\"Text Correction and Auto-completion\")\n",
    "\n",
    "        with demo:\n",
    "            gr.Markdown(\"# Spell Checker and Auto-completion\")\n",
    "            gr.Markdown(f\"Dictionary loaded with {len(self.processor.dictionary)} words. \"\n",
    "                        f\"{self.processor.ngram_size}-gram model with {len(self.processor.ngram_counts)} contexts.\")\n",
    "\n",
    "            with gr.Tab(\"Text correction\"):\n",
    "                with gr.Row():\n",
    "                    text_input1 = gr.Textbox(label=\"Text to correct\", lines=3,\n",
    "                                           placeholder=\"Enter text with some spelling errors...\")\n",
    "                    text_output1 = gr.Textbox(label=\"Corrected text\", lines=3)\n",
    "                correct_btn = gr.Button(\"Correct the text\")\n",
    "                correct_btn.click(fn=self.processor.correct_text, inputs=text_input1, outputs=text_output1)\n",
    "\n",
    "            with gr.Tab(\"Auto-completion\"):\n",
    "                with gr.Row():\n",
    "                    text_input2 = gr.Textbox(label=\"Beginning of text\", lines=2,\n",
    "                                           placeholder=\"Enter the beginning of a sentence...\")\n",
    "                    slider = gr.Slider(minimum=1, maximum=10, value=5, step=1,\n",
    "                                     label=\"Number of words to generate\")\n",
    "                    text_output2 = gr.Textbox(label=\"Completed text\", lines=3)\n",
    "                complete_btn = gr.Button(\"Complete the text\")\n",
    "                complete_btn.click(fn=self.processor.autocomplete_text,\n",
    "                                 inputs=[text_input2, slider], outputs=text_output2)\n",
    "\n",
    "            with gr.Tab(\"Correction + Auto-completion\"):\n",
    "                with gr.Row():\n",
    "                    text_input3 = gr.Textbox(label=\"Text with errors\", lines=3,\n",
    "                                           placeholder=\"Enter text with errors...\")\n",
    "                    slider2 = gr.Slider(minimum=1, maximum=10, value=5, step=1,\n",
    "                                      label=\"Number of words to generate\")\n",
    "                with gr.Row():\n",
    "                    corr_output = gr.JSON(label=\"Results\")\n",
    "                process_btn = gr.Button(\"Correct and complete\")\n",
    "                process_btn.click(fn=self.processor.correct_and_autocomplete,\n",
    "                                inputs=[text_input3, slider2], outputs=corr_output)\n",
    "\n",
    "            with gr.Tab(\"Text generation\"):\n",
    "                with gr.Row():\n",
    "                    text_input4 = gr.Textbox(label=\"Beginning of the text\", lines=2,\n",
    "                                           placeholder=\"Enter the beginning of a text...\")\n",
    "                    slider3 = gr.Slider(minimum=5, maximum=50, value=20, step=5,\n",
    "                                      label=\"Maximum length (words)\")\n",
    "                    text_output4 = gr.Textbox(label=\"Generated text\", lines=5)\n",
    "                generate_btn = gr.Button(\"Generate the continuation\")\n",
    "                generate_btn.click(fn=self.processor.generate_continuation,\n",
    "                                 inputs=[text_input4, slider3], outputs=text_output4)\n",
    "\n",
    "        return demo\n",
    "\n",
    "    def launch(self, **kwargs):\n",
    "        \"\"\"Launch the Gradio interface.\"\"\"\n",
    "        self.demo.launch(**kwargs)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the application.\"\"\"\n",
    "\n",
    "    # Replace these paths with your actual file paths\n",
    "\n",
    "    corpus_path = \"/content/sample_data/big_data.txt\"\n",
    "    keyboard_graph_path = \"/content/sample_data/qwerty_graph.txt\"\n",
    "\n",
    "    # Create the text processor\n",
    "    processor = TextProcessor(\n",
    "        corpus_path=corpus_path,\n",
    "        keyboard_graph_path=keyboard_graph_path,\n",
    "        ngram_size=2,\n",
    "        smoothing_k=0.1,\n",
    "        min_frequency=2\n",
    "    )\n",
    "\n",
    "    # Create and launch the app\n",
    "    app = TextProcessorApp(processor)\n",
    "    app.launch(share=True)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
